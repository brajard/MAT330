\ifdefined\ishandout
\documentclass[handout]{beamer}
\else
\documentclass{beamer}
\fi

%\usepackage[frenchb]{babel}
\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{tikz}
\usepackage{framed}
\usepackage{xmpmulti}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{color, colortbl}
\ifdefined\ishandout
\usepackage{handoutWithNotes}
\fi
\usepackage{slashbox}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{hhline}
\usepackage{pgfplots}
\usepackage{caption}

\def\UrlBreaks{\do\/\do-}

\usetikzlibrary{shapes.geometric}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.arrows, chains}
\usetikzlibrary{arrows,calc}
\usetikzlibrary{shapes.multipart}
\usetikzlibrary{matrix}

\usepackage{array}
%\usetheme{Boadilla}
\usetheme[progressbar=frametitle]{metropolis}

\usefonttheme[onlymath]{serif}

\newcommand{\R}{\mathbb{R}}
%\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\ifdefined\ishandout
\pgfpagesuselayout{3 on 1 with notes}[a4paper,border shrink=5mm]
\usecolortheme{dove}
\else
%\usecolortheme{dolphin}
%\usecolortheme{crane}
\fi

\metroset{block=fill}

\lstnewenvironment{codeC}
{ \lstset{language=C,
    otherkeywords={printf,scanf}}
}
{}

\ifdefined\ishandout
\definecolor{mygreen}{rgb}{0,0,0}
\definecolor{mymauve}{rgb}{0,0,0}
\definecolor{myblue}{rgb}{0,0,0}
\else
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{myblue}{rgb}{0,0,1}

\fi

%% Notes
%\setbeameroption{show only notes}


\definecolor{mygray}{rgb}{0.5,0.5,0.5}

\lstset{ language=Python,%
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=tb,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=none,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=3,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}
%\lstset{language=Python,
% breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
%  breaklines=true,                 % sets automatic line breaking
%  captionpos=b,                
%%commentstyle=\itshape\color{mymauve},
%%keywordstyle=\bfseries\color{myblue},
%numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
%  numbersep=8pt,                   % how far the line-numbers are from the code
%  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
%%  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
%  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
%%  showstringspaces=false,          % underline spaces within strings only
%  showtabs=false,                  % show tabs within strings adding particular underscores
%  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
%%  stringstyle=\color{mygreen},     % string literal style
%  tabsize=2 
%}
\ifdefined\ishandout
\newcommand{\red}{\textbf}
\else
\newcommand{\red}{\textcolor{red}}
\fi
%\newcommand \emph
%Default size : 12.8 cm * 9.6 cm

\newcommand{\tmark}[1]{\tikz[remember picture, baseline=-.5ex]{\coordinate(#1);}}

\definecolor{bluegreen}{RGB}{0,149,182}


%\newcommand{\output}[1]{
\setbeamertemplate{navigation symbols}{}
\newcommand{\bvrb}{\Verb[commandchars=£µ§,formatcom=\color{bluegreen}]}
\newcommand{\footvrb}{\footnotesize\Verb}
\newcommand{\vrbalert}[2][]{\visible<#1>{#2}}
%%% Commande pour les listes/arbres
\newcommand{\mvide}{\nodepart{one} \nodepart{two}}
\newcommand{\tvide}{\nodepart{one} \nodepart{two} \nodepart{three}}
\newcommand{\rref}[1][]{\hfill{\scriptsize\textit{#1}}}

%%Fin des commandes pour les listes/arbres.



%%% Paramètres du cours (à régler)
%Numéro du cours
\newcommand{\nb}{1}

\title[Machine Learning]{Machine learning and physical modelling-2}
\author[J. Brajard]{julien.brajard@nersc.no}
\institute[NERSC]{NERSC\\
\url{https://github.com/brajard/MAT330}}
\date{October 2021}

\begin{document}
%%%%%%%%%%%%%%%%%%%%% SLIDES DE TITRE
\begin{frame}
\titlepage
%\centering{
%\url{http://australe.upmc.fr} (onglet EPU-C5-IGE Info Gen)}
\end{frame}

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{A standard Machine learning model: Random Forests}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{A decision tree}
    Predict house price (in \$1000's) from 13 features:
    \begin{figure}
        \centering
        \includegraphics[width=.9\textwidth]{fig/L2/tree.png}
    \end{figure}
        
    \begin{table}
    \centering
    \begin{tabular}{c|l}
       CRIM   &  per capita crime rate by town \\   
       NOX    &  nitric oxides concentration  \\
       RM     &  average number of rooms per dwelling \\
       DIS    &  distance to employment centres \\
       LSTAT  &  lower status of the population \\
         
        \end{tabular}
        \end{table}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Uni-variate example}
\begin{columns}
\column{.5\textwidth}
\begin{figure}
    \centering
    \includegraphics<1>[width=\textwidth]{fig/L2/tree-data.png}
    \includegraphics<2>[width=\textwidth]{fig/L2/tree-2.png}
    \includegraphics<3>[width=\textwidth]{fig/L2/tree-5.png}
\end{figure}
    \column{.5\textwidth}
\begin{figure}
    \centering
    \includegraphics<2>[width=\textwidth]{fig/L2/tree_uni_2.png}
    \includegraphics<3>[width=\textwidth]{fig/L2/tree_uni_5.png}
\end{figure}
\end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{From tree to forest}
Disadvantages of regression tree:
\begin{itemize}
    \item Can overfit the data
\end{itemize}
\visible<2->{
One extension of Regression Tree:
\alert{Random Forest}}
\begin{figure}
    \centering
    \includegraphics<1>[width=.7\textwidth]{fig/L2/tree.jpg}
    \includegraphics<2>[width=.7\textwidth]{fig/L2/forest.jpeg}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{The (over simplified) principle of Random Forest}
\begin{tikzpicture}[node distance = 0.5 cm]
\draw [very thick, fill=gray] (0,0) rectangle (3,6);
\node (data) at (1.5,6.2) {Data};
\pause
%TREE 1
\def\xx{4}
\def\yy{4.5}
\foreach \y in {0.5,2,3,3.5,4.5}
{
\draw[orange,fill=orange] (0,\y) rectangle +(3,.5);
}
\draw [very thick] (0,0) rectangle (3,6);

\draw [black,fill=orange] (\xx,\yy) rectangle +(1,1.5);
\path [->,draw] (3,3) -- (\xx,{\yy + 0.75});
\pause
\node [anchor=west] (t1) at ({\xx+2},{\yy + 0.75}) {Tree 1};
\path [->,draw] ({\xx+1},{\yy + 0.75}) -- node [above] {fit} ({\xx+2},{\yy + 0.75}) ;

\pause
%TREE 2
\def\xx{4}
\def\yy{2.5}

\draw [very thick, fill=gray] (0,0) rectangle (3,6);

\foreach \y in {0.,1,2,2.5,3.5,4.5}
{
\draw[blue,fill=blue] (0,\y) rectangle +(3,.5);
}
\draw [very thick] (0,0) rectangle (3,6);

\draw [black,fill=blue] (\xx,\yy) rectangle +(1,1.5);
\path [->,draw] (3,3) -- (\xx,{\yy + 0.75});
\node [anchor=west] (t2) at ({\xx+2},{\yy + 0.75}) {Tree 2};
\path [->,draw] ({\xx+1},{\yy + 0.75}) -- node [above] {fit} ({\xx+2},{\yy + 0.75}) ;

\pause
%TREE 3
\def\xx{4}
\def\yy{0.5}

\draw [very thick, fill=gray] (0,0) rectangle (3,6);

\foreach \y in {0.5,1.5,2,3.5,5.5}
{
\draw[red,fill=red] (0,\y) rectangle +(3,.5);
}
\draw [very thick] (0,0) rectangle (3,6);

\draw [black,fill=red] (\xx,\yy) rectangle +(1,1.5);
\path [->,draw] (3,3) -- (\xx,{\yy + 0.75});
\node [anchor=west] (t3)  at ({\xx+2},{\yy + 0.75}) {Tree 3};
\path [->,draw] ({\xx+1},{\yy + 0.75}) -- node [above] {fit} ({\xx+2},{\yy + 0.75}) ;

\pause
\node [right = of t2](average) {\textbf{Average}};
\node [right = of average] (out) {};
\path[->,draw] (t1) -- (average) ;
\path[->,draw] (t2) -- (average) ;
\path[->,draw] (t3) -- (average) ;
\path[->,draw] (average) -- (out) ;

\end{tikzpicture}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Results on the univariate experiment}

\begin{figure}
    \centering
\caption*{
\alt<1>{Prediction of Randoms trees}{Prediction of a Random Forest}}
    \includegraphics<1>[width=.9\textwidth]{fig/L2/tree-5.png}
    \includegraphics<2>[width=.9\textwidth]{fig/L2/RF.png}

\end{figure}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Some key parameters}
\begin{lstlisting}[language=Python]
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestRegressor(n_estimators=n, max_features=maxf, min_samples_split=min_split,...)
\end{lstlisting}
\vspace{-1.5em}
\begin{itemize}[<+->]
    \item \verb|n_estimators|: number of trees (generally the larger is the better)
    \item   \verb|max_features|: number of features to consider at each split. The default number is the total number of features. A larger value makes provides a smaller bias (accuracy) but a bigger variance (risk of overfitting)
      \item \verb|min_samples_fit|: number of features to consider at each split. The minimum value of 2 means that the tree is fully developed (small bias but great variance).
\end{itemize}

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Determination of the hyperparameters}
    \begin{itemize}[<+->]
        \item Parameters that are not optimized during the training are called \alert{hyperparameters}.
        \item They can be determined using a score on the \alert{validation dataset} or using a \alert{cross-validation} procedure.
    \end{itemize}
    \pause
    \begin{columns}[b]
    \column{.42\textwidth}
      \begin{figure}
       \centering
       \includegraphics[width=\textwidth]{fig/L2/machine_learning.png}
   \end{figure}
   \column{.58\textwidth}
   \vfill
   {\footnotesize
   \url{https://xkcd.com/1838/}}
    \end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Stir the pile: The gridsearch}
\begin{enumerate}
    \item Specify a list of hyperparameters to be tested.
    \item For each of the parameters, specify a set of values to test
    \item Train a model for each of the possible combinations of hyperparameters
    \item Retain the best model (using, e.g., cross-validation)
\end{enumerate}

\begin{columns}[b]
\column{.5\textwidth}
\begin{figure}
    \centering
    \includegraphics[trim={0 0 10cm 0},clip,width=\textwidth]{fig/L2/cIDuR.png}
\end{figure}
\column{.5\textwidth}

\vfill
{\footnotesize \url{https://medium.com/@senapati.dipak97/grid-search-vs-random-search-d34c92946318}
}
\end{columns}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Remarks on the gridsearch procedure}

\begin{itemize}[<+->]
    \item It make an \alert{exhaustive} search of the hyperparameters
    \item The procedure is easy to \alert{parallelized}.
    \item it is not naturally adapted for quantitative hyperparameters.
    \item it can become \alert{very costly}. (e.g. 8 hyperparameters with 8 values each to test = $8^8=16,777,216$ trainings.
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Random search}
    \begin{enumerate}
    \item Specify a list of hyperparameters to be tested.
    \item For each of the parameters, specify a set of values to test or a law to draw a random value.
    \item Draw $n$ combinations of the hyperparameters.
    \item Train a model for each of the combinations.
    \item Retain the best model (using, e.g., cross-validation)
\end{enumerate}
\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{fig/L2/cIDuR2.png}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Remarks on the random search procedure}

\begin{itemize}
    \item It \alert{does not make} an \alert{exhaustive} search of the hyperparameters
    \item The procedure is easy to \alert{parallelized}.
    \item The cost is predictable (number of draw).
\end{itemize}
\pause
Both gridsearch and random search are implemented and easy to use in scikit-learn.
\end{frame}


\begin{frame}{Bayesian search}
A more ''advance'' method it the \alert{Bayesian}.\\

The principle is to search hyperparameters when it is more likely to improve the model (based on previous attempts)\\

Illustration taken from Ryan P. Adams: "A Tutorial on Bayesian Optimization for Machine Learning"
    
    
\end{frame}

\begin{frame}{}
    
\begin{figure}
        \centering
\multiinclude[<+->][format=png,graphics={width=\textwidth}]{fig/L2//hopt}       

    \end{figure}
    \end{frame}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural Networks}

\begin{frame}
  \frametitle{The perceptron : an artificial neuron}
  \begin{tikzpicture}[
    basic/.style={draw,fill=blue!20,text width=1em,text badly centered},
    input/.style={basic,circle},
    weights/.style={basic,rectangle},
functions/.style={basic,circle,fill=blue!10},
]
        \node[functions] (center) {};
        \node[below of=center,font=\scriptsize,text width=4em] {Activation function};
        \draw[thick] (0.5em,0.5em) -- (0,0.5em) -- (0,-0.5em) -- (-0.5em,-0.5em);
      %  \draw (0em,0.75em) -- (0em,-0.75em);
      %  \draw (0.75em,0em) -- (-0.75em,0em);
        \node[right of=center, anchor = west] (right) {$y=$ 0 ou 1};
            \path[draw,->] (center) -- (right);
        \node[functions,left=3em of center] (left) {$\sum$};
            \path[draw,->] (left) -- (center);
%        \node[weights,left=3em of left] (2) {$w_2$} -- (2) 
            \node[input,left= 4em of left] (l2) {$x_2$};
%            \path[draw,->] (l2) -- (2);
            \path[draw,->] (l2) -- node[above,midway]{$w_2$}(left);
        \node[below of=l2] (dots) {$\vdots$} ;
%(dots) node[left of=dots] (ldots) {$\vdots$};
%        \node[weights,below of=dots] (n) {$w_n$} -- (n) 
\node[input,below of=dots] (ln) {$x_n$};
%            \path[draw,->] (ln) -- (n);
            \path[draw,->] (ln) -- node[above,midway]{$w_n$}(left);
%        \node[weights,above of=2] (1) {$w_1$} -- (1) 
            \node[input,above of=l2] (l1) {$x_1$};
%            \path[draw,->] (l1) -- (1);
            \path[draw,->] (l1) -- node[above,midway]{$w_1$}(left);
%        \node[weights,above of=1] (0) {$w_0$} -- (0) 
\node[input,above of=l1] (l0) {$1$};
%            \path[draw,->] (l0) -- (0);
            \path[draw,->] (l0) -- node[above,midway]{$w_0$}(left);
        \node[below of=ln,font=\scriptsize](lin) {inputs};
        \node[right of=lin,xshift=+1em,font=\scriptsize] {weights};
    \end{tikzpicture}
\begin{block}{Computation}
$
y = f\left( w_0 + w_1. x_1 + w_2. x_2 +  \dots + w_n. x_n \right) = f\left( w_0 + \sum_{i=1}^n w_i.x_i\right)
$
\end{block}
\end{frame}

\begin{frame}{Some remarks}
\begin{itemize}[<+->]
    \item Inputs $x_i$ are the different features of the data
    \item Weight $w_i$ are the parameters of the model to optimize
    \item If the activation function is identity, it is equivalent to a linear regression
\end{itemize}
\visible<4->{
\begin{block}{}
More complexe models are build by combining several perceptrons
\end{block}
}

\end{frame}


\begin{frame}{Multi-layer perceptron (Densely connected layers)}
    \def\layersep{1.5cm}

\begin{tikzpicture}[
   shorten >=1pt,->,
   draw=black!50,
    node distance=\layersep,
    every pin edge/.style={<-,shorten <=1pt},
    neuron/.style={circle,fill=black!25,minimum size=17pt,inner sep=0pt},
    input neuron/.style={neuron, fill=green!50},
    output neuron/.style={neuron, fill=red!50},
    hidden neuron/.style={neuron, fill=blue!50},
    annot/.style={text width=4em, text centered}
]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,4}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Feature \#\y] (I-\name) at (0,-\y) {};

    % set number of hidden layers
    \newcommand\Nhidden{3}

    % Draw the hidden layer nodes
    \foreach \N in {1,...,\Nhidden} {
       \foreach \y in {1,...,5} {
          \path[yshift=0.5cm]
              node[hidden neuron] (H\N-\y) at (\N*\layersep,-\y cm) {};
           }
    \node[annot,above of=H\N-1, node distance=1cm] (hl\N) {Hidden layer \N};
    }

    % Draw the output layer node
    \node[output neuron,pin={[pin edge={->}]right:Output}, right of=H\Nhidden-3] (O) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,4}
        \foreach \dest in {1,...,5}
            \path (I-\source) edge (H1-\dest);

    % connect all hidden stuff
    \foreach [remember=\N as \lastN (initially 1)] \N in {2,...,\Nhidden}
       \foreach \source in {1,...,5}
           \foreach \dest in {1,...,5}
               \path (H\lastN-\source) edge (H\N-\dest);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,5}
        \path (H\Nhidden-\source) edge (O);

    % Annotate the layers

    \node[annot,left of=hl1] {Input layer};
    \node[annot,right of=hl\Nhidden] {Output layer};
\end{tikzpicture}
\end{frame}

\begin{frame}{Most usual activation functions}
\begin{columns}
\column{.5\textwidth}
    \begin{figure}
        \centering
        Linear\\
        \includegraphics[width=.85\textwidth]{fig/L2/activ-linear.png}\\
        ReLU\\
        \includegraphics[width=.85\textwidth]{fig/L2/activ-relu.png}\\

    \end{figure}
\column{.5\textwidth}
    \begin{figure}
     \centering
        Hyperbolic tangent\\
        \includegraphics[width=.85\textwidth]{fig/L2/activ-tanh.png}\\
       Sigmoid\\
        \includegraphics[width=.85\textwidth]{fig/L2/activ-sigmoid.png}\\

    \end{figure}
\end{columns}
\end{frame}


\begin{frame}{New fancy activation functions}
\begin{columns}
\column{.5\textwidth}
    \begin{figure}
        \centering
        Leaky-ReLU\\
        \includegraphics[width=.85\textwidth]{fig/L2/activ-l-relu.png}\\
      

    \end{figure}
\column{.5\textwidth}
    \begin{figure}
     \centering
        Swish\\
        \includegraphics[width=.85\textwidth]{fig/L2/activ-swish.png}\\
     

    \end{figure}
\end{columns}
\end{frame}


\begin{frame}{Classification and regression loss}
  
   \begin{columns}
   \column{.5\textwidth}
   \alert{Regression}
   \begin{itemize}
       \item Last layer:\\
       linear or hyperbolic tangent
       \item Loss function:\\
       $$L(\hat{y},y) = \sum_i (\hat{y}_i-y_i)^2$$
   \end{itemize}
   \pause
\column{.5\textwidth}
       \alert{Classification}
   \begin{itemize}
       \item Last layer:\\
       Soft-max
       $$
       p_j=f_j(\mathbf{h}) = \frac{e^{h_j}}{\sum_k e^{h_k}}
       $$
       
       \item Loss function:\\
       Negative crossentropy
       $$L(p,y) = -\sum_i \sum_j y_{i,j}.\log{p_{i,j}}$$
   \end{itemize}
   \end{columns}
\end{frame}

\begin{frame}{Classification loss (binary case)}
Objective: binary classification (the model determine if the input feature is in a class or not)\\
\vspace{1em}

Exemple: Try to know if an image is a cat or not.\\
\vspace{1em}

Dataset:

\begin{table}[]
    \centering
    \begin{tabular}{ccccc}
        {\bf x (feature)} &
         \includegraphics[width=.2\textwidth]{fig/L2/cat1.jpg}& 
         \includegraphics[width=.2\textwidth]{fig/L2/dog1.jpg}&
         \includegraphics[width=.2\textwidth]{fig/L2/cat2.jpg}&
         ...
         \\
        {\bf y (target)} & Cat & Dog & Cat & ...\\
    \end{tabular}
\end{table}

\end{frame}

\begin{frame}{How to proceed?}

\begin{enumerate}[<+->]
    \item Encode the targets (1 for cat, 0 otherwise)
    \begin{table}[]
    \centering
    \begin{tabular}{ccccc}
    
        {\bf y (target)} & Cat & Dog & Cat & ...\\
         {\bf y (encoded)} & 1 & 0 & 1 & ...\\
        
    \end{tabular}
\end{table}
    \item Design a model with \alert{\bf one output } and use the \alert{\bf sigmoid} as output activation function of your model $f(x)$, so $0 < f(x) < 1$.\\
    Rule: if $f(x)>\frac{1}{2}$, classify as ''Cat''.\\
    \alert{\bf f(x) is interpreted as the probability of the image x to be a cat.}
    \item Loss function to minimize is binary cross entropy:
    $$
    L = -\sum y_i \cdot \log(f(x_i)) + (1-y_i)\cdot \log (1-f(x_i))
    $$
\end{enumerate}    
\end{frame}

\begin{frame}{Classification loss (multiclass)}
Objective:  classification (the model determine in which class the input feature (more than 2 classes))\\
\vspace{1em}

Exemple: Try to know if an image is a cat, a dog or a duck.\\
\vspace{1em}

Dataset:

\begin{table}[]
    \centering
    \begin{tabular}{ccccc}
        {\bf x (feature)} &
         \includegraphics[width=.2\textwidth]{fig/L2/cat1.jpg}& 
         \includegraphics[width=.2\textwidth]{fig/L2/dog1.jpg}&
         \includegraphics[width=.2\textwidth]{fig/L2/duck1.jpg}&
         ...
         \\
        {\bf y (target)} & Cat & Dog & Duck & ...\\
    \end{tabular}
\end{table}

\end{frame}

\begin{frame}{How to proceed?}

\begin{enumerate}[<+->]
    \item Encode the targets using one hot encoding
    \begin{table}[]
    \centering
    \begin{tabular}{ccccc}
    
        {\bf y (target)} & Cat & Dog & Cat & ...\\
         {\bf y (encoded)} & $(1,0,0)$ & $(0,1,0)$ & $(0,0,1)$ & ...\\
        
    \end{tabular}
\end{table}
    \item Design a model with \alert{\bf $N$ outputs } ($N$ being the numner of modalities) and use the \alert{\bf soft-max} as output activation function
    $$
       p_j=f_j(\mathbf{h}) = \frac{e^{h_j}}{\sum_k e^{h_k}}
    $$
    Rule the class is attributed to the argument of the maximum of ${\bf p}$. Ex ${\bf p} = (0.1, 0,7, 0,2)$ is classified as ''dog''.
    \alert{\bf $p_j$ is interpreted as the probability of the image x to belong to the class $j$}
    \item Loss function to minimize is negative cross entropy
 $$L = -\sum_i \sum_j y_{i,j}.\log{p_{i,j}}$$
\end{enumerate}    
\end{frame}


\begin{frame}[fragile]{Convolutional neural net}
    \begin{tikzpicture}
    \draw[step=0.8,color=gray] (0,0) grid (4.8,4.8);
    \matrix[matrix of nodes,
    inner sep=0pt,
    anchor=south west,
    nodes={inner sep=0pt,text width=.8cm,align=center,minimum height=0.8cm}
    ]{
    $x_{11}$ & $x_{12}$ & $x_{13}$& $x_{14}$ & $x_{15}$  & $x_{16}$ \\
     $x_{21}$ & $x_{22}$ & $x_{23}$& $x_{24}$ & $x_{25}$ &  $x_{26}$ \\
    $x_{31}$ &$x_{32}$ & $x_{33}$& $x_{34}$ & $x_{35}$  & $x_{36}$ \\
    $x_{41}$ &$x_{42}$ & $x_{43}$& $x_{44}$ & $x_{45}$ & $x_{46}$  \\
     $x_{51}$ &$x_{52}$ & $x_{53}$& $x_{54}$ & $x_{55}$&  $x_{56}$  \\
     $x_{61}$ &$x_{62}$ & $x_{63}$& $x_{64}$ & $x_{65}$&  $x_{66}$  \\
    };
    \node (in) at (4.8,2.4) {} ;
    \node (out) at (8,2.4) {} ;
    \draw[step=0.8,color=gray] (8,0.8) grid (11.2,4);
    \matrix[matrix of nodes,
    xshift=8cm,
    yshift=0.8cm,
    inner sep=0pt,
    anchor=south west,
    nodes={inner sep=0pt,text width=.8cm,align=center,minimum height=0.8cm}
    ]{
    $h_{11}$ & $h_{12}$ & $h_{13}$& $h_{14}$ \\
     $h_{21}$ & $h_{22}$ & $h_{23}$& $h_{24}$ \\
    $h_{31}$ &$h_{32}$ & $h_{33}$& $h_{34}$   \\
    $h_{41}$ &$h_{42}$ & $h_{43}$& $h_{44}$  \\
    };
    
    \draw[step=0.5,color=gray] (5.999,3.499) grid (7.5,5);
    \matrix[matrix of nodes,
    xshift=5.999cm,
    yshift=3.499cm,
    inner sep=0pt,
    anchor=south west,
    nodes={inner sep=0pt,fill=orange!20,font=\footnotesize,text width=.5cm,align=center,minimum height=0.5cm}
    ]{
    $w_{11}$ & $w_{12}$ & $w_{13}$ \\
    $w_{21}$ & $w_{22}$ & $w_{23}$ \\
    $w_{31}$ & $w_{32}$ & $w_{33}$ \\
    };
    
    \path [draw, very thick, ->] (in) edge[bend left=40] 
    node [above,midway]{$\mathbf{w}$}(out);
    \node (xlabel) at (2.4,5) {$X$: an image};
    \node (hlabel) at (9.6,4.2) {$h$: first feature};
    \end{tikzpicture}
    \begin{block}{Perform a standard convolution}
    $$
    h_{i,j} = \sum_{k=1}^3\sum_{l=1}^3 x_{i+k-1,j+l-1}.w_{k,l}
    $$
    \end{block}
\end{frame}


\begin{frame}[t]{Main parameters of a convolutional layer}
\begin{itemize}
    \item <1-> \alert<1> {Size of the filter} $K$ 
    \item <2-> \alert<2> {Number of filters} $p$

\only<2>{
A convolutional layer is composed of $p$ convolutions (size of layer) extracting $p$ features from the data.
\begin{figure}
\centering
    \includegraphics[width=.6\textwidth,trim={0 3.5cm 12.5cm 5cm},clip]{fig/L2/Cnn-layer.png}
\end{figure}
}

    \item <3-> \alert<3> {Strides} $S$
    
    \only<3> {
{\centering
\begin{tabular}{cc}
    $S=1$ & $S=2$ \\
    \includegraphics[trim={0.5cm 0 2.5cm 0}, clip, width=.5\textwidth]{fig/L2/Stride1.png}&
    \includegraphics[trim={0 0 2.5cm 0}, clip, width=.5\textwidth]{fig/L2/Stride2.png}\\
    \end{tabular}
    }}
    
    \item <4-> \alert<4> {Padding} $P$
    
    {\only<4>
    {\centering
\begin{tabular}{cc}
    $P=1$ & $P=2$ \\
    \includegraphics[trim={0 0 1.5cm 0}, clip, width=.45\textwidth]{fig/L2/pad3.png}&
    \includegraphics[trim={0 0 1.5cm 0}, clip, width=.45\textwidth]{fig/L2/pad5.png}\\
    \end{tabular}
    }

}
    
\end{itemize}
    \pause
    {\footnotesize
    $O = \frac{W - K +2P}{S} + 1$, where $O$ is the output size and $W$ the input size.
    }
\end{frame}


\begin{frame}{Summary of Convolutional layer steps}
    \begin{figure}
        \centering
        1. Convolution
\multiinclude[<+->][format=png,graphics={width=\textwidth}]{fig/L2/animated/conv}       
    \end{figure}
    \pause
    \vspace{-2em}
    \begin{columns}[t]
    \column{.7\textwidth}
    \begin{figure}
        \centering
        2. Addition
            \multiinclude[<+->][format=png,graphics={width=\textwidth}]{fig/L2/animated/channels}       

    \end{figure}
    
       \column{.3\textwidth}
       \pause
    \begin{figure}
        \centering
        3. Bias\\
\multiinclude[<+->][format=png,graphics={width=.6\textwidth}]{fig/L2/animated/bias}       
    \end{figure}
    \end{columns}
\end{frame}

\begin{frame}{Remarks on Convolutional layers}
\begin{itemize}
    \item Convolutional layers are acting locally on the image (But you can still use large scale information by adding more layers)
    \item Convolutions are invariant by translation (the weights do not depend on the location on the image).
    \item They can handle images of different sizes.
    
\end{itemize}
    
\end{frame}
\begin{frame}{Max-Pooling}
    In order to reduce the size of the feature space (en to enhance the gradients), a common operation is to perform a max-pooling.
    \begin{figure}
   \centering
    \includegraphics[width=.6\textwidth]{fig/L2/MaxpoolSample2.png}
\end{figure}
\end{frame}

\begin{frame}[fragile]{A traditionnal CNN architecture}
    \begin{figure}
\centering
    \includegraphics[width=.8\textwidth,trim={0 0cm 0cm 3cm},clip]{fig/L2/CNN.png}
\end{figure}
\end{frame}

\begin{frame}{Example of AlexNet}
    \alert{AlexNet} is the first Deep architecture used on ImageNet challenge in 2012 and achieved an \alert{error of 15.3\%} (10\% better than the previous best classifier). The paper was cited more than 34,000 times.

\begin{thebibliography}{GBC16}

\bibitem[KH12]{Krizhevsky2012ImageNetNetworks}
Alex Krizhevsky and Geoffrey~E Hinton, \emph{{ImageNet Classification with Deep
  Convolutional Neural Networks}}, Neural Information Processing Systems
  (2012), 1--9.

\end{thebibliography}
    \begin{figure}
        \centering
        \includegraphics[width=.5\textwidth]{fig/L2/AlexNet_Summary_Table.jpg}

    \end{figure}
\end{frame}
\section{A quick typology of few neural nets}
%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Recurrent Neural Networks}
    \begin{figure}
        \centering
        \includegraphics[width=.4\textwidth]{fig/L2/rnn.png}

    \end{figure}

Some popular types of recurrent neural networks:
\begin{itemize}
    \item Long short-term memory (LSTM)
    \item Gated Reccurent Unit (GRU)
\end{itemize}
\alert{Used in machine translation and text processing}
\end{frame}

\begin{frame}{Autoencoders}
    \begin{figure}
        \centering
        \includegraphics[width=.9\textwidth]{fig/L2/AE.png}

    \end{figure}
\alert{Used in image denoising, compressing, generation,...}
    
\end{frame}

\begin{frame}{Generative adversarial networks}
    \begin{figure}
        \centering
    \includegraphics[width=.9\textwidth]{fig/L2/GAN.png}
  \end{figure}
    
\end{frame}

\begin{frame}{Residual Networks}
    \begin{columns}
    \column{.5\textwidth}
        \begin{figure}
        \centering
        \includegraphics[width=.9\textwidth]{fig/L2/res_block.png}

    \end{figure}
    $x$~: input, $y$~: output
    $$
    y = x + \mathcal{F}(x) 
    $$
    \pause
    \column{.5\textwidth}
            \begin{figure}
        \centering
        \includegraphics[height=.9\textheight,trim={4cm 0cm 0cm 0cm},clip]{fig/L2/resnet_full.png}

    \end{figure}
    \end{columns}
\end{frame}



\section{L1/L2 regularization}

\begin{frame}
\frametitle{Example of a non-linear relashonship}
\begin{figure}
\includegraphics[height=0.55\textheight]{./fig/L1/scatter.png}
\end{figure}
\begin{block}{An idea}
We could take an polynomial hypothesis model:
$$
h_{\bm{\theta}}(x) = \theta_0 x^0 + \theta_1 x^1 + \ldots + \theta_p x^p
$$
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example}
$\{(x_1,y_1),\ldots,(x_n,y_n)\}$ is the learning dataset.

For a given polynomial degree $p$, parameters $\bm{\theta}$ are determined minimizing the 
least-mean square cost function:
$$
J(\bm{\theta}) = \frac{1}{n} \sum (y_i - h_{\bm{\theta}}(x_i))^2
$$
with $h_{\bm{\theta}}(x) = \theta_0 x^0 + \theta_1 x^1 + \ldots + \theta_p x^p$
\begin{itemize}[<+->]
\item It can be determined using a gradient descent method
\item If the degree of the polynomial $p=1$, it is a simple linear regression
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{A first result}
\vspace{-2em}
\begin{columns}[t]
\column{.5\textwidth}
\begin{figure}
$p=1$ (linear regression)\\
\includegraphics[width=0.99\textwidth]{./fig/L1/linreg_pow1.png}
\end{figure}
\column{.5\textwidth}
\begin{figure}
residuals\\
\includegraphics[width=0.99\textwidth]{./fig/L1/residuals.png}
\end{figure}

\end{columns}
\begin{block}{Prediction error}
$$
\text{err} = \frac{1}{n}\sum \text{res}^2 = 5.46e-2
$$
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Increasing the polynomial degree ?}
\begin{columns}
\column{.5\textwidth}
\begin{figure}
$p=3$ (cubic regression)\\
\includegraphics[width=0.99\textwidth]{./fig/L1/linreg_pow3.png}
\end{figure}
\column{.5\textwidth}
\begin{block}{Prediction error}
$$
\text{err} = \frac{1}{n}\sum \text{res}^2 = 1.80e-2
$$
\end{block}
\end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Is it different from the linear regression ?}
Let's consider :
$$
\bm{x} = 
\begin{pmatrix}
1 \\
x^1 \\
x^2 \\
\vdots \\
x^p
\end{pmatrix}
$$
then
$$
h_{\bm{\theta}}(x) = \theta_0  + \theta_1 x^1 + \ldots + \theta_p x^p = \bm{\theta}^T \bm{x}
$$
\begin{alertblock}{}
By extending a scalar predictor to a vector,
polynomial regression is equivalent to linear regression.
\end{alertblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Increasing the degree ?}
\begin{columns}
\column{.33\textwidth}
\vspace{-2em}
\begin{figure}
$p=1$
\includegraphics[width=0.99\textwidth]{./fig/L1/linreg_pow1.png}
\end{figure}
\vspace{-2em}
\begin{figure}
$p=9$
\includegraphics[width=0.99\textwidth]{./fig/L1/linreg_pow9.png}
\end{figure}
\column{.33\textwidth}
\vspace{-2em}
\begin{figure}
$p=3$
\includegraphics[width=0.99\textwidth]{./fig/L1/linreg_pow3.png}
\end{figure}
\vspace{-2em}
\begin{figure}
$p=12$
\includegraphics[width=0.99\textwidth]{./fig/L1/linreg_pow12.png}
\end{figure}
\column{.33\textwidth}
\vspace{-2em}
\begin{figure}
$p=4$
\includegraphics[width=0.99\textwidth]{./fig/L1/linreg_pow4.png}
\end{figure}
\vspace{-2em}
\begin{figure}
$p=15$
\includegraphics[width=0.99\textwidth]{./fig/L1/linreg_pow15.png}
\end{figure}
\end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Overfitting}
\begin{block}{}
When there is to many paramaters to fit, the model can reproduce 
a random noise, it is called \alert{overfitting}
\end{block}
\pause
\begin{table}
\resizebox{\textwidth}{!}{%
\input{./coeflin}}
\end{table}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{High parameters values}
\begin{figure}
Value of the parameters $|\theta_1|$ with respect with the degree of the polynomial
regression\\
\includegraphics[height=0.75\textheight]{./fig/L1/coefs_th1.png}
\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Regularization}
\begin{block}{The idea}
The idea of regularization is to perform a regression minimizing a cost function
that includes a term to penalize "big" values for the parameters:
$$
J(\bm{\theta}) = \frac{1}{n} \sum (y_i - h_{\bm{\theta}}(x_i))^2 + \alpha P(\bm{\theta})
$$
\end{block}
\pause
We consider two penalty terms:
\begin{itemize}[<+->]
\item \alert{Ridge Regularization (L2):} $P(\bm{\theta}) = \sum_{i=0}^p \theta_i^2$
\item \alert{Lasso Regularization (L1):} $P(\bm{\theta}) = \sum_{i=0}^p |\theta_i|$
\item Elastic Net combines both regularization
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Ridge regression (L2)}
\begin{block}{}
Ridge regression is a linear regression with a Ridge regularization:
$$
J(\bm{\theta}) = \frac{1}{n} \sum (y_i - h_{\bm{\theta}}(x_i))^2 + \alpha \sum_{i=0}^p \theta_i^2
$$
\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Results for $p=15$ and varying $\alpha$}
\begin{columns}
\column{.33\textwidth}
\vspace{-2em}
\begin{figure}
$\alpha=0$
\includegraphics[width=0.99\textwidth]{./fig/L1/ridge_alpha0.png}
\end{figure}
\vspace{-2em}
\begin{figure}
$\alpha=1e-3$
\includegraphics[width=0.99\textwidth]{./fig/L1/ridge_alpha1e-3.png}
\end{figure}
\column{.33\textwidth}
\vspace{-2em}
\begin{figure}
$\alpha=1e-15$
\includegraphics[width=0.99\textwidth]{./fig/L1/ridge_alpha1e-15.png}
\end{figure}
\vspace{-2em}
\begin{figure}
$\alpha=1e-2$
\includegraphics[width=0.99\textwidth]{./fig/L1/ridge_alpha1e-2.png}
\end{figure}
\column{.33\textwidth}
\vspace{-2em}
\begin{figure}
$\alpha=1e-4$
\includegraphics[width=0.99\textwidth]{./fig/L1/ridge_alpha1e-4.png}
\end{figure}
\vspace{-2em}
\begin{figure}
$\alpha=5$
\includegraphics[width=0.99\textwidth]{./fig/L1/ridge_alpha5.png}
\end{figure}
\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Values of coefficients}
\vspace{-2em}
\begin{table}
\resizebox{\textwidth}{!}{%
\input{./coefridge}}
\end{table}
\begin{figure}
\includegraphics[height=0.4\textheight]{./fig/L1/coefs_th5_ridge.png}\\
Value of $\theta_5$
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Determination of the parameters in the ridge regression}
Considering the cost function J:
$$
J(\bm{\theta}) =J_{lms}(\bm{\theta}) + \alpha \sum_{i=0}^p \theta_i^2
$$
In a gradient algorithm, update of the parameters:

$\theta^{k+1}_i = \theta^k_i - \nu.(\frac{\partial J_{lms}}{D\theta_i} - 2\alpha\theta^k_i)$

So the update rule is:
$$
\theta^{k+1}_i = \theta^k_i(1-2 \nu \alpha) - \Delta_{lms}
$$
where
$\Delta_{lms}$ is the update in case of non-regularized regression

\end{frame}


%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Lasso regression (L1)}
\begin{block}{}
Lasso regression is a linear regression with a Lasso regularization:
$$
J(\bm{\theta}) = \frac{1}{n} \sum (y_i - h_{\bm{\theta}}(x_i))^2 + \alpha \sum_{i=0}^p |\theta_i|
$$
\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Results for $p=15$ and varying $\alpha$}
\begin{columns}
\column{.33\textwidth}
\vspace{-2em}
\begin{figure}
$\alpha=0$
\includegraphics[width=0.99\textwidth]{./fig/L1/lasso_alpha0.png}
\end{figure}
\vspace{-2em}
\begin{figure}
$\alpha=1e-3$
\includegraphics[width=0.99\textwidth]{./fig/L1/lasso_alpha1e-3.png}
\end{figure}
\column{.33\textwidth}
\vspace{-2em}
\begin{figure}
$\alpha=1e-15$
\includegraphics[width=0.99\textwidth]{./fig/L1/lasso_alpha1e-15.png}
\end{figure}
\vspace{-2em}
\begin{figure}
$\alpha=1e-2$
\includegraphics[width=0.99\textwidth]{./fig/L1/lasso_alpha1e-2.png}
\end{figure}
\column{.33\textwidth}
\vspace{-2em}
\begin{figure}
$\alpha=1e-4$
\includegraphics[width=0.99\textwidth]{./fig/L1/lasso_alpha1e-4.png}
\end{figure}
\vspace{-2em}
\begin{figure}
$\alpha=5$
\includegraphics[width=0.99\textwidth]{./fig/L1/lasso_alpha5.png}
\end{figure}
\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Values of coefficients}
\vspace{-2em}
\begin{table}
\resizebox{\textwidth}{!}{%
\input{./coeflasso}}
\end{table}
\begin{figure}
\includegraphics[height=0.4\textheight]{./fig/L1/coefs_th5_lasso.png}\\
Value of $\theta_5$
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Determination of the parameters in the lasso regression}
Considering the cost function J:
$$
J(\bm{\theta}) =J_{lms}(\bm{\theta}) + \alpha \sum_{i=0}^p |\theta_i|
$$
In a gradient algorithm, update of the parameters would be:

J is not differentiable.
If we consider $\theta_{lms} = \theta_i^k -\nu.\frac{\partial J_{lms}}{D\theta_i} $

The update rule is:
\begin{itemize}
\item $\theta_i^{k+1} = \theta_{lms} + \alpha \nu$  if $\theta_{lms} < -\alpha \nu$
\item $\theta_i^k = 0$ if $-\alpha \nu < \theta_{lms} < \alpha \nu$
\item $\theta_i^{k+1} = \theta_{lms} - \alpha \nu$  if $\theta_{lms} > \alpha \nu$
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Summary of parameters updates}
\begin{figure}
\includegraphics[height=0.8\textheight]{./fig/L1/updates.png}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Comparison Ridge/Lasso}
\begin{exampleblock}{Ridge (L2)}
\begin{itemize}
\item Prevents the overfitting
\item includes all the features (dimensions) of the predictor, 
so it can be useless for high dimensional predictors
\end{itemize}
\end{exampleblock}
\pause
\begin{alertblock}{Lasso (L1)}
\begin{itemize}
\item Provides sparse solutions and reduce the dimension of the
predictor
\item If some features in the predictor are correlated, arbitrarily select
one from the others.
\end{itemize}
\end{alertblock}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{In a neural network}
L2 (Ridge) and L1 (Lasso) regularization are widely use in Neural Network architectures.
\begin{figure}
    \centering
 \begin{tikzpicture}[%
    node distance = 2.5em,
    basic/.style={draw,fill=blue!20,text width=1em,text badly centered},
    input/.style={basic,circle,fill=green!20},
    output/.style={basic,circle,fill=red!20},
    weights/.style={basic,rectangle},
functions/.style={basic,circle,fill=blue!10}
]
        \node[] (center) {};
        \node[right = of center, anchor = west,output] (right) {$\hat{y}$};
        \node[above of=center,functions] (h1) {$h_1$};
        \node[below of=center,functions] (h2) {$h_2$};
        \path[draw,->] (h1) -- node[above,midway]{$w^{1}_1$}(right);
        
        \path[draw,->] (h2) --  node[below,midway]{$w^{1}_2$}(right);
        
        \node[left = of h1, input] (x1) {$x_1$};
        \node[left = of h2, input] (x2) {$x_2$};
        
        \path [draw,->] (x1) -- node[above,midway]{$w^0_{11}$} (h1);
        \path [draw,->] (x1) -- node[pos=0.25,above]{$w^0_{12}$} (h2);
        \path [draw,->] (x2) -- node[pos=0.05,above]{$w^0_{21}$} (h1);
        \path [draw,->] (x2) -- node[below,midway]{$w^0_{22}$} (h2);
\end{tikzpicture}
\end{figure}
\begin{itemize}
    \item Non-regularized loss function: $L(\mathbf{w}) = ||\hat{y}(\mathbf{w})-y||^2$.
    \item L2-regularized loss function: $L(\mathbf{w}) = ||\hat{y}(\mathbf{w})-y||^2 + \alpha \sum_{i=0}^p |w_i|$  .
    \item L1-regularized loss function: $L(\mathbf{w}) = ||\hat{y}(\mathbf{w})-y||^2 + \alpha \sum_{i=0}^p w_i^2$. 
\end{itemize}
    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Wrapping-up}
\begin{exampleblock}{Advantage}
L1/L2 regularization prevents overfitting even for large architecture (a big weight vector to optimize)
\end{exampleblock}
\vspace{2em}
\pause
\begin{alertblock}{But...}
It comes with extra hyperparameter to tune (using, e.g., cross-validation): $\alpha$
\end{alertblock}
\end{frame}

%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Questions addressed in this lecture}
    \begin{itemize}
        \item What is the principle of the Random Forests? [Van16,5.8]
        \item How to determine the hyperparamters? [Van16,5.3]
        \item What is a artificial neural network? [GBC16,6]
        \item What is a convolutive layers [GBC16,9]
        \item What is L1(Lasso)/L2(Ridge) regularization? [Van16,5.6; GBC16,8]
            {\footnotesize
\href{https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1}{https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1}}
        \item What are the main types of neural networks? [GBC16,10, 14, 20]
    \end{itemize}

\begin{footnotesize}
\begin{block}{Refs}
~[Van16,$n$]: Jake VanderPlas, \textit{Python Data Science Handbook}, section $n$\\
~[GBC16,$n$]: Goodfellow etal., Deep Learning, chapter $n$
\end{block}
\end{footnotesize}
\end{frame}


\end{document}


\begin{frame}{Training a neural-net: gradient backpropagation}

\begin{columns}

\column{.5\textwidth}
\begin{figure}
    \centering
 \begin{tikzpicture}[%
    node distance = 2.5em,
    basic/.style={draw,fill=blue!20,text width=1em,text badly centered},
    input/.style={basic,circle,fill=green!20},
    output/.style={basic,circle,fill=red!20},
    weights/.style={basic,rectangle},
functions/.style={basic,circle,fill=blue!10}
]
        \node[] (center) {};
        \node[right = of center, anchor = west,output] (right) {$\hat{y}$};
        \node[above of=center,functions] (h1) {$h_1$};
        \node[below of=center,functions] (h2) {$h_2$};
        \path[draw,->] (h1) -- node[above,midway]{$w^{1}_1$}(right);
        
        \path[draw,->] (h2) --  node[below,midway]{$w^{1}_2$}(right);
        
        \node[left = of h1, input] (x1) {$x_1$};
        \node[left = of h2, input] (x2) {$x_2$};
        
        \path [draw,->] (x1) -- node[above,midway]{$w^0_{11}$} (h1);
        \path [draw,->] (x1) -- node[pos=0.25,above]{$w^0_{12}$} (h2);
        \path [draw,->] (x2) -- node[pos=0.05,above]{$w^0_{21}$} (h1);
        \path [draw,->] (x2) -- node[below,midway]{$w^0_{22}$} (h2);
\end{tikzpicture}
\end{figure}
\begin{block}{Objective}
Determination of the best set of weights $\mathbf{w}$ to minimize the Loss function $L(\mathbf{w}) = ||\hat{y}(\mathbf{w})-y||^2$.\
\alert{Calculation of $\partial L/\partial w$}
\end{block}
\column{.6\textwidth}
\begin{enumerate}[<+->]
    \item Given a couple $(x,y)$
    \item \alert{Forward computation:}\\
    $h_j  =  f_0(\sum_{i=1}^2 w^0_{ij}.x_i)$\\
$\hat{y}  =  f_1(\sum_{j=1}^2 w^1_j. h_j)$
\item \alert{Compute the gradient of the loss:~}
$\boxed{\color{red}\partial L/\partial \hat{y}}$

    \item \alert{Gradient Backpropagation:}
    \begin{itemize}
   \item Layer 1\\
   $\alert{\partial L/\partial w_j^1} = 
   \boxed{\color{red}\partial L/\partial \hat{y}}.
   \partial f_1 / \partial w^1_j$\\
   
   $ \boxed{\color{blue}\partial L/\partial h_j} = 
    \boxed{\color{red}\partial L/\partial \hat{y}}. 
   \partial f_1 / \partial h_j$
   
   \item Layer 0\\
   $\alert{\partial L/\partial w^0_{ij}}=
  \boxed{\color{blue}\partial L/\partial h_j}
   .\partial f_1 / \partial w^0_{ij}  $
    
    \end{itemize}
\end{enumerate}

\end{columns}

\end{frame}



\section{Optimizing a machine learning (gradient method)}

\begin{frame}{Optimizing the loss}
    Several loss function (depending on the problem) can be defined.
    
    For example, Mean Square Error:
    
    \begin{alertblock}{Method}
    Find a minimum of L by adjustig the parameters (weights) $\mathbf{w}$ given the gradient of the loss with respect to the weights $\nabla_\mathbf{w}L$.
    \end{alertblock}
\end{frame}

\begin{frame}{Batch gradient}
    
\end{frame}

\begin{frame}{Remarks on batch gradient}
    
\end{frame}

\begin{frame}{Scaling the data}
    
\end{frame}

\begin{frame}{Pure stochastic gradient}
    
\end{frame}

\begin{frame}{Remarks on pure stochastic gradient}
    
\end{frame}

\begin{frame}{Mini-batch gradient}
    
\end{frame}

\begin{frame}{Batch/Stochastic training}
Dataset: $(X,y)$ with N samples, $\mathbf{w}$: initial weights\\~\\
\begin{columns}[t]
\column{.5\textwidth}
\alert{Batch Training:}
\begin{itemize}
    \item For $i$ from $1$ to $N$:\\
    \begin{enumerate}
    \item    $L = L + L(f(x_i),y_i)$
     \end{enumerate}
    \item Calculate $\partial L/\partial w$
    \item update weights:$\mathbf{w}$
    
\end{itemize}
\alert{1 Update is performed after N forward passes of the neural net.}
\pause
\column{.5\textwidth}
\alert{Stochastic Training:}
\begin{itemize}
 \item For $i$ from $1$ to $N$:\\
     \begin{enumerate}

    \item Compute $L(f(x_i),y_i)$\\
    \item Calculate $\partial L/\partial w$\\
    \item update weights:$\mathbf{w}$
    \end{enumerate}
\end{itemize}

\alert{N Updates are performed after N forward passes of the neural net.}
\end{columns}
    
\end{frame}

\begin{frame}{Batch training}
Dataset: $(X,Y)$ with N samples denoted $(\mathbf{x_i},y_i)$

\begin{columns}[t]
\column{.56\textwidth}
\begin{footnotesize}
\begin{block}{Batch gradient:}\end{block}
    \begin{algorithmic}
    \Require{Learning rate(s): $\nu_k$}
    \Require{Initial weights: $\mathbf{w}$}
    \State $k \leftarrow 1$
    \While {stopping criterion not met}
   % \Require{NNNN}
    %\State Sample $m$ examples ($\mathbf{x}_i,y_i)$ from ($X,y$)
    \State Compute gradient: $\mathbf{g} \leftarrow \frac{1}{N}\sum_i^N\nabla_\mathbf{w}L(f(\mathbf{x}_i,y_i))$
    \State Update weights: $\mathbf{w} \leftarrow \mathbf{w} - \nu_k\mathbf{g}$
    \State $k \leftarrow k + 1$
    \EndWhile 
    \end{algorithmic}
    \end{footnotesize}
    \alert{1 Update / N forwards}
    \pause
    \column{.56\textwidth}
\begin{footnotesize}
\begin{block}{Stochastic gradient:}\end{block}
    \begin{algorithmic}
    \Require{Learning rate(s): $\nu_k$}
    \Require{Initial weights: $\mathbf{w}$}
    \State $k \leftarrow 1$
    \While {stopping criterion not met}
   % \Require{NNNN}
    %\State Sample $m$ examples ($\mathbf{x}_i,y_i)$ from ($X,y$)
    \State Sample an example ($\mathbf{x},y)$ from ($X,Y$)
    \State Compute gradient: $\mathbf{g} \leftarrow \nabla_\mathbf{w}L(f(\mathbf{x},y))$
    \State Update weights: $\mathbf{w} \leftarrow \mathbf{w} - \nu_k\mathbf{g}$
    \State $k \leftarrow k + 1$
    \EndWhile 
    \end{algorithmic}
    \end{footnotesize}
    \alert{1 Update / 1 forward}
    
    
    \end{columns}
\end{frame}

\begin{frame}{Mini-Bactch training}
Dataset: $(X,y)$ with N samples
\begin{footnotesize}
    \begin{algorithmic}
    \Require{Learning rate(s): $\nu_k$}
    \Require{Initial weights: $\mathbf{w}$}
    \State $k \leftarrow 1$
    \While {stopping criterion not met}
   % \Require{NNNN}
    \State Sample $m$ examples ($\mathbf{x}_i,y_i)$ from ($X,y$)
    \State Compute gradient: $\mathbf{g} \leftarrow \frac{1}{m}\sum_i^m\nabla_\mathbf{w}L(f(\mathbf{x}_i,y_i)$
    \State Update weights: $\mathbf{w} \leftarrow \mathbf{w} - \nu_k\mathbf{g}$
    \State $k \leftarrow k + 1$
    \EndWhile 
    
    \end{algorithmic}
    \end{footnotesize}
\end{frame}

\begin{frame}{Mini-Batch training}
Dataset: $(X,y)$ with N samples, $\mathbf{w}$: initial weights\\~\\
\begin{itemize}
    \item for $k$ from $1$ to  $N//B$:
\begin{itemize}
 \item for $i$ from $B(k-1)+1$ to $Bk$:\\
     \begin{enumerate}
    \item Compute $L(f(x_i),y_i)$\\
    \end{enumerate}
    \item Calculate $\partial L/\partial w$\\
    \item update weights:$\mathbf{w}$
    \end{itemize}
\end{itemize}
N//B updates are performed after N forward passes of the neural net\\
\alert{B is the batchsize}
\begin{itemize}
    \item \alert{B=1}: stochastic training
    \item \alert{B=N}: batch training
    \item Generally B<<N
\end{itemize} 

    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



